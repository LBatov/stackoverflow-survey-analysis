{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need OHE for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./data/processed/X_train.csv', na_filter=False)\n",
    "X_val = pd.read_csv('./data/processed/X_val.csv', na_filter=False)\n",
    "X_test = pd.read_csv('./data/processed/X_test.csv', na_filter=False)\n",
    "\n",
    "y_train = pd.read_csv('./data/processed/y_train.csv', na_filter=False)\n",
    "y_val = pd.read_csv('./data/processed/y_val.csv', na_filter=False)\n",
    "y_test = pd.read_csv('./data/processed/y_test.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_names = X_train.columns\n",
    "dataset = pd.concat([X_train, X_test])\n",
    "numeric_features = ['YearsCode', 'YearsCodePro', 'Age', 'Age1stCode']\n",
    "categorical_features = list(filter(lambda col: col not in numeric_features, X_train.columns))\n",
    "categorical_features_indices = list(filter(lambda idx: idx not in [4, 5], list(range(51))))\n",
    "\n",
    "dataset = dataset[feature_names].values\n",
    "val_dataset = X_val.copy()\n",
    "val_dataset = val_dataset[feature_names].values\n",
    "train_dataset = X_train.copy()\n",
    "train_dataset = train_dataset[feature_names].values\n",
    "test_dataset = X_test.copy()\n",
    "test_dataset = test_dataset[feature_names].values\n",
    "categorical_names = {} \n",
    "les={} \n",
    "for feature in categorical_features_indices:\n",
    "    le = LabelEncoder() \n",
    "    le.fit(dataset[:, feature])\n",
    "    dataset[:, feature] = le.transform(dataset[:, feature])\n",
    "    val_dataset[:, feature] = le.transform(val_dataset[:, feature])\n",
    "    train_dataset[:, feature] = le.transform(train_dataset[:, feature])\n",
    "    test_dataset[:, feature] = le.transform(test_dataset[:, feature])\n",
    "    les[feature] = le\n",
    "\n",
    "dataset = dataset.astype('float32') \n",
    "val_dataset = val_dataset.astype('float32') \n",
    "train_dataset = train_dataset.astype('float32') \n",
    "test_dataset = test_dataset.astype('float32')\n",
    "encoder = ColumnTransformer([(\"enc\", OneHotEncoder(),\n",
    "                            categorical_features_indices)],\n",
    "                            remainder = 'passthrough')\n",
    "\n",
    "encoder.fit(dataset)\n",
    " \n",
    "encoded_dataset = encoder.transform(dataset)\n",
    "encoded_train = encoder.transform(train_dataset)\n",
    "encoded_val = encoder.transform(val_dataset)\n",
    "encoded_test = encoder.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(160, input_dim=156, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(40, kernel_initializer='random_normal', activation='relu'))\n",
    "model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "\n",
    "model.compile(loss='mae', optimizer='Adadelta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 67439.5469 - val_loss: 67599.8672\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5391 - val_loss: 67599.8672\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5078 - val_loss: 67599.8672\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5391 - val_loss: 67599.8672\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5469 - val_loss: 67599.8672\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5391 - val_loss: 67599.8672\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5234 - val_loss: 67599.8672\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5391 - val_loss: 67599.8672\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5156 - val_loss: 67599.8672\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5156 - val_loss: 67599.8672\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8672\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5391 - val_loss: 67599.8594\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5078 - val_loss: 67599.8594\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5234 - val_loss: 67599.8594\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8594\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8516\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5234 - val_loss: 67599.8516\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5312 - val_loss: 67599.8438\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5234 - val_loss: 67599.8438\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5000 - val_loss: 67599.8438\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5000 - val_loss: 67599.8438\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5156 - val_loss: 67599.8359\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67439.5000 - val_loss: 67599.7969\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 67439.4375 - val_loss: 67599.7422\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 67439.3281 - val_loss: 67599.5312\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67438.8203 - val_loss: 67598.4297\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 67432.4453 - val_loss: 67569.3672\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 48408.3398 - val_loss: 32883.7422\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 32980.3320 - val_loss: 32477.2520\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 32593.6738 - val_loss: 32225.9824\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 32076.4531 - val_loss: 31570.9121\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 31166.6152 - val_loss: 30117.5371\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 29373.7910 - val_loss: 29039.8281\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 29132.5430 - val_loss: 30687.9336\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 28179.8203 - val_loss: 27251.3301\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 27159.6660 - val_loss: 28165.8184\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 26076.0723 - val_loss: 25382.4180\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 25248.2031 - val_loss: 24664.9414\n",
      "Epoch 46/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 24556.8047 - val_loss: 24837.4727\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 24035.8477 - val_loss: 24538.9746\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 23760.2695 - val_loss: 23373.6406\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 23282.6699 - val_loss: 23105.6777\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 23131.5781 - val_loss: 22968.3125\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 23115.6543 - val_loss: 23056.1270\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22939.3516 - val_loss: 22841.6055\n",
      "Epoch 53/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22891.6230 - val_loss: 23109.4570\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22886.7930 - val_loss: 22796.4395\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22819.4375 - val_loss: 22853.5059\n",
      "Epoch 56/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22785.4863 - val_loss: 22787.1582\n",
      "Epoch 57/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22760.2930 - val_loss: 22747.6035\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22742.7949 - val_loss: 22773.5586\n",
      "Epoch 59/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22728.2598 - val_loss: 22734.2930\n",
      "Epoch 60/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22715.3848 - val_loss: 22756.4648\n",
      "Epoch 61/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22714.4375 - val_loss: 22706.6250\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22695.0078 - val_loss: 22784.4551\n",
      "Epoch 63/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22691.7402 - val_loss: 22688.7148\n",
      "Epoch 64/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22681.9043 - val_loss: 22689.4590\n",
      "Epoch 65/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22675.2930 - val_loss: 22694.1641\n",
      "Epoch 66/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22671.6191 - val_loss: 22691.8145\n",
      "Epoch 67/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22666.2617 - val_loss: 22675.4570\n",
      "Epoch 68/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22664.1992 - val_loss: 22676.6172\n",
      "Epoch 69/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22663.3652 - val_loss: 22672.0410\n",
      "Epoch 70/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22658.3574 - val_loss: 22669.0430\n",
      "Epoch 71/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22657.8398 - val_loss: 22669.0605\n",
      "Epoch 72/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22655.9902 - val_loss: 22669.9316\n",
      "Epoch 73/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22653.0840 - val_loss: 22668.2305\n",
      "Epoch 74/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22652.5586 - val_loss: 22672.3320\n",
      "Epoch 75/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22651.8594 - val_loss: 22670.9141\n",
      "Epoch 76/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22650.6230 - val_loss: 22666.2402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22650.2090 - val_loss: 22665.6367\n",
      "Epoch 78/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22649.1660 - val_loss: 22669.1758\n",
      "Epoch 79/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22648.4844 - val_loss: 22668.1816\n",
      "Epoch 80/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22648.2500 - val_loss: 22667.1816\n",
      "Epoch 81/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22647.5996 - val_loss: 22666.8184\n",
      "Epoch 82/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22647.0703 - val_loss: 22666.8770\n",
      "Epoch 83/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22646.5176 - val_loss: 22665.6055\n",
      "Epoch 84/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22646.0176 - val_loss: 22665.4844\n",
      "Epoch 85/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22645.6367 - val_loss: 22665.0820\n",
      "Epoch 86/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22645.3887 - val_loss: 22664.9219\n",
      "Epoch 87/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22644.9941 - val_loss: 22664.6172\n",
      "Epoch 88/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22644.7246 - val_loss: 22664.3516\n",
      "Epoch 89/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22644.3770 - val_loss: 22664.0312\n",
      "Epoch 90/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22644.1172 - val_loss: 22663.7500\n",
      "Epoch 91/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22643.8340 - val_loss: 22663.5527\n",
      "Epoch 92/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22643.5430 - val_loss: 22663.3086\n",
      "Epoch 93/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22643.2891 - val_loss: 22663.0312\n",
      "Epoch 94/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22643.0391 - val_loss: 22662.7812\n",
      "Epoch 95/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22642.7617 - val_loss: 22662.5293\n",
      "Epoch 96/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22642.5566 - val_loss: 22662.2871\n",
      "Epoch 97/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22642.3027 - val_loss: 22662.0586\n",
      "Epoch 98/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22642.0371 - val_loss: 22661.8027\n",
      "Epoch 99/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22641.8008 - val_loss: 22661.5723\n",
      "Epoch 100/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22641.5430 - val_loss: 22661.3496\n",
      "Epoch 101/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22641.3203 - val_loss: 22661.1250\n",
      "Epoch 102/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22641.0586 - val_loss: 22660.8633\n",
      "Epoch 103/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22640.8359 - val_loss: 22660.6074\n",
      "Epoch 104/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22640.5957 - val_loss: 22660.3711\n",
      "Epoch 105/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22640.3691 - val_loss: 22660.1250\n",
      "Epoch 106/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22640.1230 - val_loss: 22659.8926\n",
      "Epoch 107/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22639.8906 - val_loss: 22659.6621\n",
      "Epoch 108/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22639.6523 - val_loss: 22659.4453\n",
      "Epoch 109/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22639.4160 - val_loss: 22659.2441\n",
      "Epoch 110/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22639.1934 - val_loss: 22659.0020\n",
      "Epoch 111/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22638.9473 - val_loss: 22658.7891\n",
      "Epoch 112/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22638.7266 - val_loss: 22658.5449\n",
      "Epoch 113/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22638.4805 - val_loss: 22658.3008\n",
      "Epoch 114/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22638.2617 - val_loss: 22658.0625\n",
      "Epoch 115/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22638.0234 - val_loss: 22657.8340\n",
      "Epoch 116/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22637.7910 - val_loss: 22657.6094\n",
      "Epoch 117/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22637.5566 - val_loss: 22657.3906\n",
      "Epoch 118/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22637.3203 - val_loss: 22657.1719\n",
      "Epoch 119/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22637.0957 - val_loss: 22656.9512\n",
      "Epoch 120/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22636.8594 - val_loss: 22656.7363\n",
      "Epoch 121/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22636.6387 - val_loss: 22656.5078\n",
      "Epoch 122/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22636.3926 - val_loss: 22656.2930\n",
      "Epoch 123/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22636.1543 - val_loss: 22656.0879\n",
      "Epoch 124/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22635.9414 - val_loss: 22655.8652\n",
      "Epoch 125/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22635.7148 - val_loss: 22655.6387\n",
      "Epoch 126/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22635.4766 - val_loss: 22655.4258\n",
      "Epoch 127/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22635.2305 - val_loss: 22655.2246\n",
      "Epoch 128/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22635.0020 - val_loss: 22654.9980\n",
      "Epoch 129/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 22634.7656 - val_loss: 22654.7715\n",
      "Epoch 130/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 22634.5410 - val_loss: 22654.5430\n",
      "Epoch 131/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 22634.3008 - val_loss: 22654.3203\n",
      "Epoch 132/200\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 22634.0762 - val_loss: 22654.1016\n",
      "Epoch 133/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22633.8340 - val_loss: 22653.8652\n",
      "Epoch 134/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22633.5977 - val_loss: 22653.6230\n",
      "Epoch 135/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22633.3770 - val_loss: 22653.4062\n",
      "Epoch 136/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22633.1289 - val_loss: 22653.1680\n",
      "Epoch 137/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22632.8945 - val_loss: 22652.9629\n",
      "Epoch 138/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22632.6738 - val_loss: 22652.7441\n",
      "Epoch 139/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22632.4434 - val_loss: 22652.5254\n",
      "Epoch 140/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22632.2090 - val_loss: 22652.2910\n",
      "Epoch 141/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22631.9844 - val_loss: 22652.0898\n",
      "Epoch 142/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22631.7500 - val_loss: 22651.8652\n",
      "Epoch 143/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22631.5254 - val_loss: 22651.6621\n",
      "Epoch 144/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22631.2969 - val_loss: 22651.4219\n",
      "Epoch 145/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22631.0547 - val_loss: 22651.1836\n",
      "Epoch 146/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22630.8223 - val_loss: 22650.9688\n",
      "Epoch 147/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22630.6016 - val_loss: 22650.7539\n",
      "Epoch 148/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22630.3770 - val_loss: 22650.5312\n",
      "Epoch 149/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22630.1562 - val_loss: 22650.3145\n",
      "Epoch 150/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22629.9141 - val_loss: 22650.1035\n",
      "Epoch 151/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22629.6777 - val_loss: 22649.8730\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 2ms/step - loss: 22629.4707 - val_loss: 22649.6367\n",
      "Epoch 153/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22629.2227 - val_loss: 22649.4160\n",
      "Epoch 154/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22629.0215 - val_loss: 22649.1953\n",
      "Epoch 155/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22628.7832 - val_loss: 22648.9668\n",
      "Epoch 156/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22628.5371 - val_loss: 22648.7422\n",
      "Epoch 157/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22628.3184 - val_loss: 22648.5332\n",
      "Epoch 158/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22628.0957 - val_loss: 22648.3203\n",
      "Epoch 159/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22627.8672 - val_loss: 22648.1035\n",
      "Epoch 160/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22627.6406 - val_loss: 22647.8906\n",
      "Epoch 161/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22627.4219 - val_loss: 22647.6719\n",
      "Epoch 162/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22627.1719 - val_loss: 22647.4414\n",
      "Epoch 163/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22626.9512 - val_loss: 22647.2402\n",
      "Epoch 164/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22626.7305 - val_loss: 22647.0254\n",
      "Epoch 165/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22626.4766 - val_loss: 22646.7969\n",
      "Epoch 166/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22626.2656 - val_loss: 22646.5801\n",
      "Epoch 167/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22626.0254 - val_loss: 22646.3809\n",
      "Epoch 168/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22625.7969 - val_loss: 22646.1816\n",
      "Epoch 169/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22625.5527 - val_loss: 22646.0020\n",
      "Epoch 170/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22625.3516 - val_loss: 22645.8008\n",
      "Epoch 171/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22625.1211 - val_loss: 22645.5820\n",
      "Epoch 172/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22624.8789 - val_loss: 22645.3457\n",
      "Epoch 173/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22624.6621 - val_loss: 22645.1445\n",
      "Epoch 174/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22624.4316 - val_loss: 22644.9297\n",
      "Epoch 175/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22624.2031 - val_loss: 22644.7246\n",
      "Epoch 176/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22623.9746 - val_loss: 22644.4961\n",
      "Epoch 177/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22623.7441 - val_loss: 22644.2930\n",
      "Epoch 178/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22623.5176 - val_loss: 22644.0742\n",
      "Epoch 179/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22623.3027 - val_loss: 22643.8613\n",
      "Epoch 180/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22623.0664 - val_loss: 22643.6543\n",
      "Epoch 181/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22622.8398 - val_loss: 22643.4512\n",
      "Epoch 182/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22622.6055 - val_loss: 22643.2539\n",
      "Epoch 183/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22622.3828 - val_loss: 22643.0430\n",
      "Epoch 184/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22622.1484 - val_loss: 22642.8125\n",
      "Epoch 185/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22621.9082 - val_loss: 22642.6035\n",
      "Epoch 186/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22621.6895 - val_loss: 22642.3867\n",
      "Epoch 187/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22621.4590 - val_loss: 22642.1719\n",
      "Epoch 188/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22621.2207 - val_loss: 22641.9336\n",
      "Epoch 189/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22620.9883 - val_loss: 22641.7090\n",
      "Epoch 190/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22620.7715 - val_loss: 22641.5059\n",
      "Epoch 191/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22620.5352 - val_loss: 22641.3066\n",
      "Epoch 192/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22620.3047 - val_loss: 22641.0977\n",
      "Epoch 193/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22620.0840 - val_loss: 22640.8867\n",
      "Epoch 194/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22619.8555 - val_loss: 22640.6758\n",
      "Epoch 195/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22619.6211 - val_loss: 22640.4668\n",
      "Epoch 196/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22619.3789 - val_loss: 22640.2656\n",
      "Epoch 197/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22619.1621 - val_loss: 22640.0664\n",
      "Epoch 198/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22618.9297 - val_loss: 22639.8555\n",
      "Epoch 199/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22618.7051 - val_loss: 22639.6465\n",
      "Epoch 200/200\n",
      "87/87 [==============================] - 0s 2ms/step - loss: 22618.4805 - val_loss: 22639.4336\n"
     ]
    }
   ],
   "source": [
    "fit_model = model.fit(\n",
    "    encoded_train,\n",
    "    y_train,\n",
    "    batch_size=300,\n",
    "    epochs=200,\n",
    "    use_multiprocessing =True,\n",
    "    # Validation of loss and metrics\n",
    "    # at the end of each epoch:\n",
    "    validation_data=(encoded_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22247.720924989255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(model.predict(encoded_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BeCompact\\.conda\\envs\\lime_and_catboost\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\BeCompact\\.conda\\envs\\lime_and_catboost\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./models/keras/keras.bin\\assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model.save('./models/keras/keras.bin')\n",
    "with open('./models/keras/LabelEncoders','wb') as f:\n",
    "    pickle.dump(les,f)\n",
    "    \n",
    "with open('./models/keras/ColumnEncoder','wb') as f:\n",
    "    pickle.dump(encoder,f)\n",
    "\n",
    "with open('./models/keras/CFI','wb') as f:\n",
    "    pickle.dump(categorical_features_indices,f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = pd.read_csv('./data/processed/X_test.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./models/keras/keras.bin')\n",
    "les = {}                                \n",
    "encoder = ColumnTransformer([(\"enc\", OneHotEncoder(),\n",
    "                            categorical_features_indices)],\n",
    "                            remainder = 'passthrough')\n",
    "                                \n",
    "with open('./models/keras/LabelEncoders','rb') as f:\n",
    "    les = pickle.load(f)         \n",
    "\n",
    "with open('./models/keras/ColumnEncoder','rb') as f:\n",
    "    encoder = pickle.load(f)          \n",
    "                                \n",
    "test_dataset = X_test2.copy()\n",
    "test_dataset = test_dataset[feature_names].values\n",
    "\n",
    "\n",
    "\n",
    "for feature in categorical_features_indices:\n",
    "    test_dataset[:, feature] = le.transform(test_dataset[:, feature])\n",
    "\n",
    "test_dataset = test_dataset.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "encoded_test = encoder.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(model.predict(encoded_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
